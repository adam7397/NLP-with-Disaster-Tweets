{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-a88e2c4d8aaf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a88e2c4d8aaf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda install pandas\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import automl_v1beta1 as automl\n",
    "\n",
    "# workaround to fix gapic_v1 error\n",
    "from google.api_core.gapic_v1.client_info import ClientInfo\n",
    "\n",
    "from automlwrapper import AutoMLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes a utility script that wraps much of the AutoML Python client library, to make the code in this notebook easier to read. Feel free to check out the utility for all the details on how we are calling the underlying AutoML Client Library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own values for these. bucket_name should be the project_id + '-lcm'.\n",
    "PROJECT_ID = 'cloudml-demo'\n",
    "bucket_name = 'cloudml-demo-lcm'\n",
    "\n",
    "region = 'us-central1' # Region must be us-central1\n",
    "dataset_display_name = 'kaggle_tweets'\n",
    "model_display_name = 'kaggle_starter_model1'\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# adding ClientInfo here to get the gapic_v1 call in place\n",
    "client = automl.AutoMlClient(client_info=ClientInfo())\n",
    "\n",
    "print(f'Starting AutoML notebook at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "nlp_test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "def callback(operation_future):\n",
    "    result = operation_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data spelunking\n",
    "How often does 'fire' come up in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the presence of the word 'fire' help determine whether the tweets here are real or false?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCS upload/download utilities\n",
    "These functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print('File {} uploaded to {}'.format(\n",
    "        source_file_name,\n",
    "        'gs://' + bucket_name + '/' + destination_blob_name))\n",
    "    \n",
    "def download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n",
    "    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n",
    "    os.makedirs(destination_directory, exist_ok = True)\n",
    "    full_file_path = os.path.join(destination_directory, file_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.download_to_filename(full_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage.Bucket(storage_client, name=bucket_name)\n",
    "if not bucket.exists():\n",
    "    bucket.create(location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV and upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the text body and the target value, for sending to AutoML NL\n",
    "nlp_train_df[['text','target']].to_csv('train.csv', index=False, header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_df[['id','text','target']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gcs_path = 'uploads/kaggle_getstarted/full_train.csv'\n",
    "upload_blob(bucket_name, 'train.csv', training_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amw = AutoMLWrapper(client=client, \n",
    "                    project_id=PROJECT_ID, \n",
    "                    bucket_name=bucket_name, \n",
    "                    region='us-central1', \n",
    "                    dataset_display_name=dataset_display_name, \n",
    "                    model_display_name=model_display_name)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create (or retreive) dataset\n",
    "Check to see if this dataset already exists. If not, create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "if not amw.get_dataset_by_display_name(dataset_display_name):\n",
    "    print('dataset not found')\n",
    "    amw.create_dataset()\n",
    "    amw.import_gcs_data(training_gcs_path)\n",
    "\n",
    "amw.dataset\n",
    "print(f'Dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick off the training for the model\n",
    "And retrieve the training info after completion. \n",
    "Start model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "\n",
    "if not amw.get_model_by_display_name():\n",
    "    print(f'Training model at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "    amw.train_model()\n",
    "\n",
    "print(f'Model trained. Ensuring model is deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "amw.deploy_model()\n",
    "amw.model\n",
    "print(f'Model trained and deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amw.model_full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Note that prediction will not run until deployment finishes, which takes a bit of time.\n",
    "However, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Begin getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "\n",
    "# Create client for prediction service.\n",
    "prediction_client = automl.PredictionServiceClient()\n",
    "amw.set_prediction_client(prediction_client)\n",
    "\n",
    "predictions_df = amw.get_predictions(nlp_test_df, \n",
    "                                     input_col_name='text', \n",
    "#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n",
    "                                     limit=None, \n",
    "                                     threshold=0.5,\n",
    "                                     verbose=False)\n",
    "\n",
    "print(f'Finished getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Undeploy model\n",
    "Undeploy the model to stop charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amw.undeploy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df['class'].iloc[:10]\n",
    "# nlp_test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = submission_df.rename(columns={'class':'target'})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit predictions to the competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
